# Explainability Algorithm Parameters

# Model parameters
# Provide local paths to a classifier checkpoint after fine-tuning called model
# Or point to a model from the HuggingFace Model Hub
# The lm parameter is only used by SOC and should point to a version of
# The base language model that was fine-tuned for classification
# That has been further pretrained on the training data
# Using masked langauge modeling as described in the SOC paper.
# To just use MSP, ignore or comment out the lm parameter.
# The max_seq_len should be that of the model and tokenizer
# The class_strategy should be one of ['binary', 'multi_label', 'multi_class']
# Based on the type of classification problem
model: "aychang/roberta-base-imdb"
lm: "roberta-base"
tokenizer: "roberta-base"
max_seq_len: 512
class_strategy: "multi_class"

# Data parameters
# Provide the name of a HuggingFace dataset if offline is set to False
# Script has been tested with ['sst2', 'yelp_polarity', 'imdb']
# If offline is set to True, provide the path to a saved HuggingFace dataset
# The batch_size parameters is for tokenization only
# An improvement to this codebase would be to
# Collect sequences in batches when running inference
# The num_sample parameter is the number of documents to sample from
# The test split of the provided dataset, which is used by default
offline: False
data: "imdb"
batch_size: 32
num_sample: 5

# Label parameters
# Provide a dictionary of class indices and label names
# This should correspond to the label names in the provided dataset
idx2label:
  0: "Negative Sentiment"
  1: "Positive Sentiment"

# MSP and SOC parameters
# Note that P is only used in MSP
# There should be no real difference between parameters for MSP and SOC
# Except that for MSP, K is the size of the phrase.
# This is called the radius in the SOC paper.
# The MSP paper flips the K and M parameters when describing them,
# Refering to K in terms of the number of important blocks retrieved
# For consistency with the Precision@K metric,
# And instead using B to represent the size of the phrase/radius.
# In the code, K is the size of the phrase/radius, and M is the
# Number of important blocks to surface.
# Note that N for SOC is just the number of samples to take for each block.
# In the MSP paper, we used 100 iterations of SOC.
# The SOC paper recommended only 20 iterations (though more should be better).
# In the MSP paper, we wanted to expected number of draws for a single block
# To equal 100 (as we used 100 iterations for SOC), so we set N=1000,
# As the expected number of times a given block is masked is given by 1000 / P
# And we used P = 0.1.
# More bootstrap iterations is better.
# Use something like 10000 bootstrap iterations.
K: 5  # subwords in a masked block of text (size of phrase for MSP or radius for SOC)
P: 0.1  # probability that a block of size K is masked - For MSP, set such that expected draws for a single block is 100
N: 1000  # number of iterations to run - For MSP set such that expected draws for a single block is 100 by computing 1000 / P
M: 5  # show the M most important blocks which led to the greatest difference in predicted probability of the given label
num_bootstrap: 10000  # set run parameters to compute p values

# Quality of life parameters
# Set debug to True for full runs/real experiments
# Print progress every print_every iterations
debug: False
print_every: 100
