# Explainability Algorithm Parameters

# Model parameters
# Provide local paths to a classifier checkpoint after fine-tuning called model
# Or point to a model from the HuggingFace Model Hub
# The lm parameter is only used by SOC and should point to a version of
# The base language model that was fine-tuned for classification
# That has been further pretrained on the training data
# Using masked langauge modeling as described in the SOC paper.
# To just use MSP, ignore or comment out the lm parameter.
# The max_seq_len should be that of the model and tokenizer
# The class_strategy should be one of ['binary', 'multi_label', 'multi_class']
# Based on the type of classification problem
model: "textattack/distilbert-base-uncased-imdb"
lm: "distilbert-base-uncased"
tokenizer: "distilbert-base-uncased"
max_seq_len: 512
class_strategy: "multi_class"

# Data parameters
# Provide the name of a HuggingFace dataset if offline is set to False.
# If offline is set to True, provide the path to a saved HuggingFace dataset.
# Script has been tested with online datasets: ['sst2', 'yelp_polarity', 'imdb']
# as well as the online CSV: "https://raw.githubusercontent.com/socd06/medical-nlp/master/data/X.csv"
# If is_csv is true, instead of looking for a HuggingFace dataset,
# the program will load the CSV file provided from the internet or a local path.
# When working with HuggingFace datasets keep is_csv set to False.
# Use one_hot to one hot encode dataset labels if not already one hot encoded.
# Currently one_hot only automatically encodes labels for the multi_class class_strategy.
# If using the multi_label class strategy, provide your own multi-hot encoded data
# as the label column (dataset column should be called 'label').
# The num_sample parameter is the number of documents to sample from.
# The split_name split of the provided dataset at random.
# To use the full dataset split instead of a random sample,
# Pass -1 to num_sample and set fixed_sample to False.
# To use a fixed sample instead of a random sample,
# set fixed_sample to True and provide the desired number of samples to num_sample.
offline: False
data: "imdb"
is_csv: False
one_hot: False
batch_size: 4
split_name: 'test'
num_sample: 30
fixed_sample: False

# Label parameters
# Provide a dictionary of class indices and label names
# This should correspond to the label names in the provided dataset
idx2label:
  0: "Negative Sentiment"
  1: "Positive Sentiment"

# MSP and SOC parameters
# Note that P is only used in MSP
# There should be no real difference between parameters for MSP and SOC
# Except that for MSP, K is the size of the phrase.
# This is called the radius in the SOC paper.
# The MSP paper flips the K and M parameters when describing them,
# Refering to K in terms of the number of important blocks retrieved
# For consistency with the Precision@K metric,
# And instead using B to represent the size of the phrase/radius.
# In the code, K is the size of the phrase/radius, and M is the
# Number of important blocks to surface.
# Note that N for SOC is just the number of samples to take for each block.
# In the MSP paper, we used 100 iterations of SOC.
# The SOC paper recommended only 20 iterations (though more should be better).
# In the MSP paper, we wanted to expected number of draws for a single block
# To equal 100 (as we used 100 iterations for SOC), so we set N=1000,
# As the expected number of times a given block is masked is given by 1000 / P
# And we used P = 0.1.
# More bootstrap iterations is better.
# Use something like 10000 bootstrap iterations.
K: 5  # subwords in a masked block of text (size of phrase for MSP or radius for SOC) - ignored if by_sent_segments is True
P: 0.1  # probability that a block of size K is masked - For MSP, set such that expected draws for a single block is 100
N: 1000  # number of iterations to run - For MSP set such that expected draws for a single block is 100 by computing 1000 / P
M: 5  # show the M most important blocks which led to the greatest difference in predicted probability of the given label
num_bootstrap: 10000  # set run parameters to compute p values
by_sent_segments: True # Ignore K and instead generate segment level explanations according to sentence splits

# Quality of life parameters
# Set debug to True for full runs/real experiments
# Print progress every print_every iterations
debug: False
print_every: 100
