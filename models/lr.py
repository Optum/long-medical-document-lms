#!/usr/bin/env python
# coding: utf-8

"""
Fit Logistic Regression Models to Benchmark Datasets 

Tokenize datasets with the BigBird tokenizer and compare predictive words as in Murdoch et al. (https://arxiv.org/pdf/1801.05453.pdf) and Jin et al. (https://arxiv.org/pdf/1911.06194.pdf) generated via Logistic Regression fit as a Bag-of-Words model to the tokenized datasets to the important text blocks generated by SOC and MSP.  This automated explanation evaluation procedure approximates human validation of important text blocks under the assumption that important words generated by Logistic Regression (by ranking regression coefficients according to absolute value) will also be informative for humans in classifying text samples from each dataset.

This approach makes a few improvements over the approach used by Murdoch et al. and Jin et al. (**validate that these are different**) in that it:

1. Focuses on multi-token blocks by creating a bag of n-gram features where n is equal to k in the SOC and MSP algorithms (the number of tokens in each text block), enabling phrase-level interpretations of important text
2. Models the text at the token-level, effectively creating a bag of subword token n-grams for natural alignment with blocks of k tokens generated by MSP and SOC
3. Preferred embodiment: represent bag-of-word features at the sentence level

Like Murdoch et al. and Jin et al., we use the Bag-of-Words Logistic Regression model (at the n-gram level) to predict the labels of interest and measure the correlation between the importance scores of text blocks from SOC and the coefficients of text blocks of the same size from the logistic regression model.  We do the same for MSP to enable comparison between methods in terms of the fidelity of the explanations generated by each algorithm.
"""

import shutil
import pickle
import numpy as np
import matplotlib.pyplot as plt
from datasets import load_from_disk, load_dataset
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import f1_score
from transformers import AutoTokenizer
from utils import make_lr_model_and_target_multi_class

# Set Run Parameters
N_GRAM_RANGE = (5, 5)
MAX_SEQ_LEN = 4096
THRESHOLD = 0.5
DATA = "mimic50"  # one of ['sst2', 'yelp_polarity', 'mimic50', 'imdb']
OUTPUT_PATH = f"./lr_outputs_{DATA}/"  # will be deleted if it already exists
OUTPUT_DATA_NAME = (
    f"ngram_range_{N_GRAM_RANGE[0]}_{N_GRAM_RANGE[1]}_features_coefs_count_vec_no_reg"
)


# Create Directory to Save Results
# This script is for demo purposes and **will delete** the `OUTPUT_PATH` directory if it exists on each new run.
# Save important results elsewhere.
if os.path.exists(OUTPUT_PATH) and os.path.isdir(OUTPUT_PATH):
    shutil.rmtree(OUTPUT_PATH)
os.makedirs(OUTPUT_PATH)

# Load Dataset
dataset = load_from_disk(
    f"/mnt/azureblobshare/hf_datasets/{DATA}.hf"
)  # use load_dataset(DATA) to pull from HuggingFace Datasets

# Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    "/mnt/azureblobshare/models/bigbird-roberta-large/"
)

# Define Function to Tokenize Text
def tokenize_function(sample):

    return tokenizer(
        sample["text"], padding="do_not_pad", truncation=True, max_length=MAX_SEQ_LEN
    )


# Define transformation to tokenize data in batches
dataset = dataset.map(
    tokenize_function,
    batched=True,
    batch_size=1024,  # just for data prep purposes
    remove_columns=["text"],
)

# Convert Integer Subword Tokens to Strings for Word Vectorizer
train_df = dataset["train"].to_pandas()
test_df = dataset["test"].to_pandas()
train_df["input_ids"] = train_df["input_ids"].apply(
    lambda seq: " ".join([str(x) for x in seq])
)
test_df["input_ids"] = test_df["input_ids"].apply(
    lambda seq: " ".join([str(x) for x in seq])
)

# Convert Dataset to Numpy Arrays
X_train = train_df["input_ids"].values
y_train = np.stack(train_df["label"].values)
X_test = test_df["input_ids"].values
y_test = np.stack(test_df["label"].values)

# View Sequence Length Distribution
lens = [len(seq.split(" ")) for seq in X_train]
plt.hist(lens, bins=20)
plt.ylabel("Frequency")
plt.xlabel("Tokens in Text Snippet")
plt.title("Distribution of Tokens per Text Snippet")
plt.tight_layout()
plt.savefig("seq_len_dist.png", transparent=False)

# Fit Pipeline to Transform Features and Fit Model
# Only search for regularization strength if not using vanilla LR
def fit_pipeline(
    X_train, y_train, clf, gs, n_splits=5, n_jobs=10, scoring="accuracy", pre_dispatch=1
):

    word_vectorizer = CountVectorizer(
        strip_accents="unicode",
        analyzer="word",
        token_pattern=r"\w{1,}",
        ngram_range=N_GRAM_RANGE,
        min_df=0.0001,
        max_df=0.2,
    )

    estimator = Pipeline([("vectorizer", word_vectorizer), ("clf", clf)])

    if gs:

        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=666)

        param_grid = {
            "clf__C": [
                0.0001,
                0.001,
                0.01,
                0.1,
                0.25,
                0.5,
                0.75,
                1,
                5,
                10,
                25,
                50,
                75,
                100,
            ],
        }

        gs = GridSearchCV(
            estimator=estimator,
            cv=cv,
            param_grid=param_grid,
            scoring=scoring,
            n_jobs=n_jobs,
            pre_dispatch=pre_dispatch,
            refit=True,
            verbose=2,
        )

        gs.fit(X_train, y_train)

        scores = gs.cv_results_["mean_test_score"]
        best_score = round(gs.best_score_, 5)
        best_params = gs.best_params_
        best_clf = gs.best_estimator_

        print("Best score: {}".format(best_score))
        print("Best parameters: {}".format(best_params))

        return best_clf

    else:

        estimator.fit(X_train, y_train)

        return estimator


# Fit vanilla logistic regression
# Regularization will screw up coefficient interpretation
clf = LogisticRegression(
    solver="sag", fit_intercept=True, max_iter=5000, class_weight=None, penalty="none"
)

# The MIMIC50 dataset is multi-label
if DATA == "mimic50":
    clf, y_train = make_lr_model_and_target_multi_class(
        clf, y_train, class_strategy="multi_class", n_jobs=10
    )

best_estimator = fit_pipeline(X_train, y_train, clf, gs=False)

# Define Function to Compute Performance Metrics
def compute_bootstrap_metrics(labels, preds, n_bootstrap=1000, avg="micro"):

    accs = []
    f1s = []
    for i in range(n_bootstrap):

        sample_indices = np.random.choice(len(labels), len(labels))
        sample_labels = labels[sample_indices]
        sample_preds = preds[sample_indices]

        acc = np.mean(sample_labels == sample_preds)
        accs.append(acc)

        f1 = f1_score(y_true=sample_labels, y_pred=sample_preds, average=avg)
        f1s.append(f1)

    return np.mean(accs), np.std(accs), np.mean(f1s), np.std(f1s)


# Compute Predicted Class Labels
if DATA == "mimic50":
    scores = best_estimator.predict_proba(X_test)
    preds = np.array((scores >= THRESHOLD), dtype=int)
    avg = "micro"
else:
    preds = np.array(
        [int(x > THRESHOLD) for x in best_estimator.predict_proba(X_test)[:, 1]]
    )
    avg = "binary"

# Measure Performance
acc_mean, acc_std, f1_mean, f1_std = compute_bootstrap_metrics(
    labels=y_test, preds=preds, avg=avg
)
acc = f"Accuracy: {acc_mean} +- {1.96 * acc_std}\n"
f1 = f"F1: {f1_mean} +- {1.96 * f1_std}\n"
print(acc)
print(f1)

# Save Performance
with open(os.path.join(OUTPUT_PATH, OUTPUT_DATA_NAME + ".txt"), "w+") as f:
    f.write(acc)
    f.write(f1)

# Extract Final Word Vectorizer and Model
if DATA != "mimic50":
    word_vectorizer, lr_model = best_estimator[0], best_estimator[1]
else:
    word_vectorizer, lr_model = best_estimator[0], best_estimator[1]

# Decode N-Gram Token Features
all_decoded_features = []
for n_gram in word_vectorizer.get_feature_names_out():
    decoded_features = tokenizer.decode(
        [int(x) for x in n_gram.split(" ")], skip_special_tokens=True
    )  # double check skipping special tokens
    all_decoded_features.append(decoded_features)

# Sort Features by Coefficient Value
pairs = list(zip(all_decoded_features, lr_model.coef_[0]))
sorted_pairs = sorted(pairs, key=lambda tup: tup[1])

# Save Features and Weights and Test Loading the Saved File
with open(os.path.join(OUTPUT_PATH, OUTPUT_DATA_NAME + ".pkl"), "wb") as f:
    pickle.dump(sorted_pairs, f)

with open(os.path.join(OUTPUT_PATH, OUTPUT_DATA_NAME + ".pkl"), "rb") as f:
    sorted_pairs = pickle.load(f)

# View Positive and Negative Predictors
print("Top negative predictors:")
print(sorted_pairs[0:100])
print(" ")
print("Top positive predictors:")
print(sorted_pairs[-100:])
