#!/usr/bin/env python
# coding: utf-8

"""
Fit Logistic Regression Models to Benchmark Datasets

Tokenize datasets with the BigBird tokenizer and compare predictive words as in Murdoch et al. (https://arxiv.org/pdf/1801.05453.pdf)
and Jin et al. (https://arxiv.org/pdf/1911.06194.pdf) generated via Logistic Regression fit as a Bag-of-Words model to the
tokenized datasets to the important text blocks generated by SOC and MSP.
This automated explanation evaluation procedure approximates human validation of important text blocks under the assumption
that important words generated by Logistic Regression (by ranking regression coefficients according to absolute value)
will also be informative for humans in classifying text samples from each dataset.

This approach makes a few improvements over the approach used by Murdoch et al. and Jin et al. in that it:

1. Focuses on multi-token blocks by creating a bag of n-gram features where n is equal to k in the SOC and MSP algorithms (the number of tokens in each text block), enabling phrase-level interpretations of important text
2. Models the text at the token-level, effectively creating a bag of subword token n-grams for natural alignment with blocks of k tokens generated by MSP and SOC

Like Murdoch et al. and Jin et al., we use the Bag-of-Words Logistic Regression model (at the n-gram level)
to predict the labels of interest and measure the correlation between the importance scores of text blocks from SOC
and the coefficients of text blocks of the same size from the logistic regression model.
We do the same for MSP to enable comparison between methods in terms of the fidelity of the explanations generated by each algorithm.
"""

import os
import yaml
import shutil
import pickle
import logging
import numpy as np
import matplotlib.pyplot as plt
from datasets import load_from_disk, load_dataset
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import f1_score
from transformers import AutoTokenizer
from utils import make_lr_model_and_target_multi_class

def main():

    # Load Run Parameters
    with open("lr_params.yml", "r") as stream:
        PARAMS = yaml.safe_load(stream)

    # Define Logger
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # Define output path and output data name
    output_path = f"./lr_outputs_{PARAMS['data']}/"  # will be deleted if it already exists
    output_data_name = (
        f"ngram_range_{PARAMS['n_gram_range_min']}_{PARAMS['n_gram_range_max']}_features_coefs_count_vec_no_reg"
    )

    # Create Directory to Save Results
    # This script is for demo purposes and **will delete** the `output_path` directory if it exists on each new run.
    # Save important results elsewhere.
    if os.path.exists(output_path) and os.path.isdir(output_path):
        shutil.rmtree(output_path)
    os.makedirs(output_path)

    # Load Data, Tokenizer, and Model
    if PARAMS["offline"]:
        os.environ["HF_DATASETS_OFFLINE"] = "1"
        dataset = load_from_disk(PARAMS["data"])
    else:
        dataset = load_dataset(PARAMS["data"])

    # Load Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(PARAMS["tokenizer"])

    # Define Function to Tokenize Text
    def tokenize_function(sample):

        return tokenizer(
            sample["text"], padding="do_not_pad", truncation=True, max_length=PARAMS['max_seq_len']
        )


    # Define transformation to tokenize data in batches
    dataset = dataset.map(
        tokenize_function,
        batched=True,
        batch_size=1024,  # just for data prep purposes
        remove_columns=["text"],
    )

    # Convert Integer Subword Tokens to Strings for Word Vectorizer
    train_df = dataset["train"].to_pandas()
    test_df = dataset["test"].to_pandas()
    train_df["input_ids"] = train_df["input_ids"].apply(
        lambda seq: " ".join([str(x) for x in seq])
    )
    test_df["input_ids"] = test_df["input_ids"].apply(
        lambda seq: " ".join([str(x) for x in seq])
    )

    # Convert Dataset to Numpy Arrays
    X_train = train_df["input_ids"].values
    y_train = np.stack(train_df["label"].values)
    X_test = test_df["input_ids"].values
    y_test = np.stack(test_df["label"].values)

    # View Sequence Length Distribution
    lens = [len(seq.split(" ")) for seq in X_train]
    plt.hist(lens, bins=20)
    plt.ylabel("Frequency")
    plt.xlabel("Tokens in Text Snippet")
    plt.title("Distribution of Tokens per Text Snippet")
    plt.tight_layout()
    plt.savefig("seq_len_dist.png", transparent=False)

    # Fit Pipeline to Transform Features and Fit Model
    # Only search for regularization strength if not using vanilla LR
    def fit_pipeline(
        X_train, y_train, clf, gs, n_splits=5, n_jobs=10, scoring="f1", pre_dispatch=1
    ):

        word_vectorizer = CountVectorizer(
            strip_accents="unicode",
            analyzer="word",
            token_pattern=r"\w{1,}",
            ngram_range=(PARAMS['n_gram_range_min'], PARAMS['n_gram_range_max']),
            min_df=0.00001,
            max_df=0.2,
        )

        estimator = Pipeline([("vectorizer", word_vectorizer), ("clf", clf)])

        if gs:

            cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=666)

            param_grid = {
                "clf__C": [
                    0.0001,
                    0.001,
                    0.01,
                    0.1,
                    0.25,
                    0.5,
                    0.75,
                    1,
                    5,
                    10,
                    25,
                    50,
                    75,
                    100,
                ],
            }

            gs = GridSearchCV(
                estimator=estimator,
                cv=cv,
                param_grid=param_grid,
                scoring=scoring,
                n_jobs=n_jobs,
                pre_dispatch=pre_dispatch,
                refit=True,
                verbose=2,
            )

            gs.fit(X_train, y_train)

            scores = gs.cv_results_["mean_test_score"]
            best_score = round(gs.best_score_, 5)
            best_params = gs.best_params_
            best_clf = gs.best_estimator_

            logger.info("Best score: {}".format(best_score))
            logger.info("Best parameters: {}".format(best_params))

            return best_clf

        else:

            estimator.fit(X_train, y_train)

            return estimator


    # Fit vanilla logistic regression
    # Regularization will screw up coefficient interpretation
    clf = LogisticRegression(
        solver="sag", fit_intercept=True, max_iter=5000, class_weight=None, penalty="none"
    )

    # The MIMIC50 dataset is multi-label
    if PARAMS['data'] == "mimic50":
        clf, y_train = make_lr_model_and_target_multi_class(
            clf, y_train, class_strategy="multi_label", n_jobs=10
        )

    best_estimator = fit_pipeline(X_train, y_train, clf, gs=False)

    # Define Function to Compute Performance Metrics
    def compute_bootstrap_metrics(labels, preds, n_bootstrap=1000, avg="micro"):

        accs = []
        f1s = []
        for i in range(n_bootstrap):

            sample_indices = np.random.choice(len(labels), len(labels))
            sample_labels = labels[sample_indices]
            sample_preds = preds[sample_indices]

            acc = np.mean(sample_labels == sample_preds)
            accs.append(acc)

            f1 = f1_score(y_true=sample_labels, y_pred=sample_preds, average=avg)
            f1s.append(f1)

        return np.mean(accs), np.std(accs), np.mean(f1s), np.std(f1s)


    # Compute Predicted Class Labels
    if PARAMS['data'] == "mimic50":
        scores = best_estimator.predict_proba(X_test)
        preds = np.array((scores >= PARAMS['threshold']), dtype=int)
        avg = "micro"
    else:
        preds = np.array(
            [int(x > PARAMS['threshold']) for x in best_estimator.predict_proba(X_test)[:, 1]]
        )
        avg = "binary"

    # Measure Performance
    acc_mean, acc_std, f1_mean, f1_std = compute_bootstrap_metrics(
        labels=y_test, preds=preds, avg=avg
    )
    acc = f"Accuracy: {acc_mean} +- {1.96 * acc_std}\n"
    f1 = f"F1: {f1_mean} +- {1.96 * f1_std}\n"
    logger.info(acc)
    logger.info(f1)

    # Save Performance
    with open(os.path.join(output_path, output_data_name + ".txt"), "w+") as f:
        f.write(acc)
        f.write(f1)

    # Extract Final Word Vectorizer and Model
    if PARAMS['data'] != "mimic50":
        word_vectorizer, lr_model = best_estimator[0], best_estimator[1]
    else:
        word_vectorizer, lr_model = best_estimator[0], best_estimator[1]

    # Decode N-Gram Token Features
    all_decoded_features = []
    for n_gram in word_vectorizer.get_feature_names_out():
        decoded_features = tokenizer.decode(
            [int(x) for x in n_gram.split(" ")], skip_special_tokens=True
        )  # double check skipping special tokens
        all_decoded_features.append(decoded_features)

    # Sort Features by Coefficient Value
    pairs = list(zip(all_decoded_features, lr_model.coef_[0]))
    sorted_pairs = sorted(pairs, key=lambda tup: tup[1])

    # Save Features and Weights and Test Loading the Saved File
    with open(os.path.join(output_path, output_data_name + ".pkl"), "wb") as f:
        pickle.dump(sorted_pairs, f)

    with open(os.path.join(output_path, output_data_name + ".pkl"), "rb") as f:
        sorted_pairs = pickle.load(f)

    # View Positive and Negative Predictors
    logger.info("Top negative predictors:")
    logger.info(sorted_pairs[0:100])
    logger.info(" ")
    logger.info("Top positive predictors:")
    logger.info(sorted_pairs[-100:])

if __name__ == "__main__":

    main()
