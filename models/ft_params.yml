# LM Fine-Tuning Parameters

# Pretrained LM
'lm_path': './bigbird-roberta-large/'

# Model name
'model_name': 'bigbird_imdb'

# Output LM
'output_path': './bigbird_imdb/'

# Data path
'data_path': './imdb.hf'

# Data params
'offline': True
'rename_text_col': False
'text_col_name': 'text'
'create_val_split': True
'val_frac_to_create': 0.1
'dataloader_num_workers': 8

# Problem type
'class_strategy': 'multi_class' # options are ['binary', 'multi_label', 'multi_class']

# Training params
'max_steps': 10000000
'per_device_train_batch_size': 4
'per_device_eval_batch_size': 4
'per_device_test_batch_size': 4
'accumulation_steps': 8 # effective batch size becomes 4 per device * 4 GPUs * 8 grad accumulation steps for 128 total
'learning_rate': 0.00005
'num_labels': 2
'max_seq_len': 4096
'metric_for_best_model': 'eval_loss'
'evaluation_strategy': 'steps'
'save_strategy': 'steps'
'eval_steps': 200
'save_steps': 200
'logging_steps': 200
'early_stopping_patience': 10
'lr_scheduler_type': 'linear'
'adam_beta1': 0.9
'adam_beta2': 0.999
'adam_epsilon': 0.00000001
'warmup_steps': 1000
'weight_decay': 0.01
'sharded_ddp': False
'gradient_checkpointing': True
'fp16': True
'fp16_eval': False
'seed': 1111
'disable_tqdm': True
'eval_accumulation_steps': 100

# Resume from checkpoint
'resume_training': False
'checkpoint': "./bigbird_imdb/checkpoint-280000/"

# Train vs test mode
'train': True
'test_model_checkpoint': "./bigbird_imdb/checkpoint-280000/"
