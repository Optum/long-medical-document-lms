{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7b1d18",
   "metadata": {},
   "source": [
    "### Evaluate Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1b2c7",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dc70f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import json\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "from datasets import load_from_disk, Dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3988f7",
   "metadata": {},
   "source": [
    "##### Evaluation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1fc80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5 # currently we don't maximize val f1 to find the threshold... need to grab scores for all the val sets if we do this\n",
    "num_std = 1.96\n",
    "num_bootstrap = 1000\n",
    "line_width = 2\n",
    "alpha = 0.2\n",
    "font_size = 16\n",
    "legend_size = 10\n",
    "x_size = 10\n",
    "y_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac2d927-76e0-48ab-8777-48bc70206d07",
   "metadata": {},
   "source": [
    "##### Initialize Score, Model, and Color Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc793b2-80d0-45ac-9a3a-15dcf8fb53fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define master lists of labels, scores, names, and colors\n",
    "all_y_trues, all_y_scores, all_model_names, all_colors = [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef4eaf2-fff5-4f83-8ade-2367a2513aa8",
   "metadata": {},
   "source": [
    "##### Load Fine-Tuned Torch LM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d15d8e-81cc-4cd5-a2dc-765325cb5a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_info = [('a', 'b', 'c'), ('x', 'y', 'z') ]\n",
    "    \n",
    "  \n",
    "for label_file, score_file, model_name in file_info:  \n",
    "    with open(label_file, \"rb\") as f:  \n",
    "        labels = pickle.load(f)  \n",
    "    with open(score_file, \"rb\") as f:  \n",
    "        scores = pickle.load(f)\n",
    "    \n",
    "    # In the case of the 2048 model, get the score for the 1 label\n",
    "    if \"RoBERTa (2048)\" in model_name:\n",
    "        scores = scores[:,1]\n",
    "      \n",
    "    all_model_names.append(model_name)  \n",
    "    all_y_trues.append(labels)  \n",
    "    all_y_scores.append(scores)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248ce15f-2ece-4a51-ab68-83889c25be80",
   "metadata": {},
   "source": [
    "##### Define Recall at Precision Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e75b9-4adc-4160-af56-9fc85a0c217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_precision(scores, labels, target_precision):\n",
    "    \n",
    "    # Compute precision-recall curve  \n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(labels, scores)  \n",
    "\n",
    "    # Find the highest recall where precision >= target_precision  \n",
    "    max_recall = recall[np.where(precision >= target_precision)].max()  \n",
    "\n",
    "    return max_recall  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baed460",
   "metadata": {},
   "source": [
    "##### Define a Function to Print the Mean and Confidence Interval for a Given Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a6d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mean_ci_of_metric_list(metric_list, metric_name, num_std):\n",
    "    mean_metric = np.mean(metric_list)\n",
    "    std_metric = np.std(metric_list)\n",
    "    metric_low = np.maximum(mean_metric - std_metric * num_std, 0)\n",
    "    metric_high = np.minimum(mean_metric + std_metric * num_std, 1)\n",
    "\n",
    "    print(\n",
    "        f\"{metric_name}: {round(mean_metric, 3)} ([{round(metric_low, 3)} - {round(metric_high, 3)}] 95% CI)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39681795-9909-4c09-b84d-f229b4663c4c",
   "metadata": {},
   "source": [
    "##### Define a Function to Select a Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc4daf-6656-4518-8f68-21ea77ab3161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold_of_best_val_f1(val_scores, val_labels):\n",
    "    \n",
    "    # Find the best threshold by maximizing F1 score\n",
    "    print(\"  Computing best threshold for F1 on validation set...\")\n",
    "    best_val_f1 = 0\n",
    "    best_threshold = 0\n",
    "    for int_threshold in range(0, 100, 1):\n",
    "        threshold = int_threshold / 100\n",
    "        sample_preds = [1 if x >= threshold else 0 for x in val_probs]\n",
    "        f1 = metrics.f1_score(y_true=val_labels, y_pred=sample_preds)\n",
    "        if f1 > best_val_f1:\n",
    "            print(f\"    Found new best F1 {f1:.4f} at threshold {threshold}\")\n",
    "            best_val_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            \n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f21cd42",
   "metadata": {},
   "source": [
    "##### Print Performance for all Metrics for all Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6ff4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_fpr_linspace = np.linspace(0, 1, 100)\n",
    "mean_recall_linspace = np.linspace(0, 1, 100)\n",
    "\n",
    "model2metric_df = {}\n",
    "for y_trues, y_scores, name in zip(\n",
    "    all_y_trues, all_y_scores, all_model_names\n",
    "):\n",
    "    accuracies, recalls, precisions, aps, interp_ps, roc_aucs, interp_tprs, f1s, rs_at_p90, static_fprs, static_tprs = [], [], [], [], [], [], [], [], [], [], []\n",
    "    for i in range(num_bootstrap):\n",
    "        \n",
    "        # Sample N records with replacement where N is the total number of records\n",
    "        sample_indices = np.random.choice(len(y_trues), len(y_trues))\n",
    "        sample_labels = np.array(y_trues)[sample_indices]\n",
    "        sample_scores = np.array(y_scores)[sample_indices]\n",
    "        \n",
    "        # Generate thresholded prediction\n",
    "        # threshold = get_threshold_of_best_val_f1(val_scores=y_val_scores, val_labels=y_val_trues)\n",
    "        sample_preds = [1 if x >= threshold else 0 for x in sample_scores]\n",
    "\n",
    "        accuracy = metrics.accuracy_score(y_true=sample_labels, y_pred=sample_preds)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "#         recall = metrics.recall_score(y_true=sample_labels, y_pred=sample_preds)\n",
    "#         recalls.append(recall)\n",
    "\n",
    "#         precision = metrics.precision_score(y_true=sample_labels, y_pred=sample_preds)\n",
    "#         precisions.append(precision)\n",
    "        \n",
    "#         f1 = metrics.f1_score(y_true=sample_labels, y_pred=sample_preds)\n",
    "#         f1s.append(f1)\n",
    "        \n",
    "        ap = metrics.average_precision_score(y_true=sample_labels, y_score=sample_scores)\n",
    "        aps.append(ap)\n",
    "        \n",
    "        p, r, thresholds = metrics.precision_recall_curve(y_true=sample_labels, probas_pred=sample_scores)\n",
    "        interp_p = np.interp(mean_recall_linspace, np.fliplr([r])[0], np.fliplr([p])[0])\n",
    "        interp_ps.append(interp_p)\n",
    "        \n",
    "        roc_auc = metrics.roc_auc_score(y_true=sample_labels, y_score=sample_scores)\n",
    "        roc_aucs.append(roc_auc)\n",
    "        \n",
    "        fpr, tpr, _ = metrics.roc_curve(y_true=sample_labels, y_score=sample_scores)\n",
    "        \n",
    "        if 'GPT-4' in name or 'Text Gen' in name:\n",
    "            static_fprs.append(fpr[1])\n",
    "            static_tprs.append(tpr[1])\n",
    "        else:\n",
    "            static_fprs.append(None)\n",
    "            static_tprs.append(None)\n",
    "        \n",
    "        interp_tpr = np.interp(mean_fpr_linspace, fpr, tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        interp_tprs.append(interp_tpr)\n",
    "        \n",
    "        r_at_p90 = recall_at_precision(scores=sample_scores, labels=sample_labels, target_precision=0.9)\n",
    "        rs_at_p90.append(r_at_p90)\n",
    "\n",
    "        # \"recalls\": recalls,\n",
    "        # \"precisions\": precisions,\n",
    "        # \"f1s\": f1s,\n",
    "        \n",
    "    metric_df = pd.DataFrame({\n",
    "        \"aps\": aps,\n",
    "        \"roc_aucs\": roc_aucs,\n",
    "    })\n",
    "    model2metric_df[name] = metric_df\n",
    "\n",
    "    print(f\"\\nResults for {name}\\n\")\n",
    "    # print_mean_ci_of_metric_list(recalls, metric_name=\"Recall\", num_std=num_std)\n",
    "    # print_mean_ci_of_metric_list(precisions, metric_name=\"Precision\", num_std=num_std)\n",
    "    # print_mean_ci_of_metric_list(f1s, metric_name=\"F1\", num_std=num_std)\n",
    "    print_mean_ci_of_metric_list(aps, metric_name=\"Average Precision\", num_std=num_std)\n",
    "    print_mean_ci_of_metric_list(roc_aucs, metric_name=\"ROC AUC\", num_std=num_std)\n",
    "    \n",
    "with open(f\"./model2metric_df.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model2metric_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8e9ab-ace6-458b-a596-eb503b3dc8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2metric_df = {k: v for k, v in model2metric_df.items() if 'Max' not in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf652e2-a0a0-435e-9e0d-80f04c6e1e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_with_95_ci(ax, data, metric, condition):      \n",
    "        \n",
    "    metric_dict = {'aps': 'PR AUC', 'roc_aucs': 'ROC AUC'}    \n",
    "    filtered_data = {k: v for k, v in data.items() if condition in k}      \n",
    "          \n",
    "    means = []      \n",
    "    errors = []      \n",
    "    colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black']      \n",
    "    for model, df in filtered_data.items():      \n",
    "        mean = df[metric].mean()      \n",
    "        std = df[metric].std()      \n",
    "        ci = 1.96 * std      \n",
    "      \n",
    "        means.append(mean)      \n",
    "        errors.append(ci)      \n",
    "      \n",
    "    y_pos = np.arange(len(filtered_data))      \n",
    "          \n",
    "    for i, model in enumerate(filtered_data.keys()):      \n",
    "        ax.barh(y_pos[i], means[i], xerr=errors[i], color=colors[i], capsize=10, label=f'M{i}: {map_model_name(model)}')      \n",
    "      \n",
    "    ax.set_yticks(y_pos)      \n",
    "    ax.set_yticklabels(['M' + str(i) for i in range(len(filtered_data))])      \n",
    "    ax.set_xlabel(metric_dict[metric])      \n",
    "    ax.set_title(f'{metric_dict[metric]} for {condition} Prediction')      \n",
    "\n",
    "conditions = ['x', 'y', 'z']      \n",
    "metrics = ['aps', 'roc_aucs']      \n",
    "    \n",
    "fig, axs = plt.subplots(3, 2, figsize=(10, 12))      \n",
    "    \n",
    "for i, condition in enumerate(conditions):      \n",
    "    for j, metric in enumerate(metrics):      \n",
    "        plot_mean_with_95_ci(axs[i][j], model2metric_df, metric, condition)      \n",
    "            \n",
    "# Add a single legend for the entire plot      \n",
    "handles, labels = axs[0][0].get_legend_handles_labels()      \n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.05),    \n",
    "          ncol=len(handles), fancybox=True, shadow=True)    \n",
    "\n",
    "# Add a single title for the entire plot  \n",
    "fig.suptitle(\"Test Set Performance (1,000 Bootstrap Iterations)\", fontsize=14, y=1.07)  \n",
    "    \n",
    "plt.tight_layout()    \n",
    "plt.subplots_adjust(top=0.99)    \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e6c0b-1aa3-4b90-bc1b-103b69d423c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
